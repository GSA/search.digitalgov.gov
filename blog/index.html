<!DOCTYPE html>
<html lang="en">
<head>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  
  <title>Search.gov Blog</title>
  <meta name="description" content="">
  <meta name="author" content="">
  <link href="https://fonts.googleapis.com/css?family=Maven+Pro:400,700" media="screen" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" media="screen" rel="stylesheet" type="text/css" />
  <link href="https://fonts.googleapis.com/css?family=Merriweather:400,700" media="screen" rel="stylesheet" type="text/css" />
  <link href="/bootstrap/css/bootstrap.css" rel="stylesheet">
  <link rel="stylesheet" href="/stylesheets/font-awesome.min.css">
  <!--[if IE 7]>
  <link rel="stylesheet" href="/stylesheets/font-awesome-ie7.min.css">
  <![endif]-->

  <!--<link href="../assets/css/bootstrap-responsive.css" rel="stylesheet">-->
  <link href="/stylesheets/custom.css" rel="stylesheet">

  <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
  <!--[if lt IE 9]>
  <script src="/javascripts/html5shiv.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <!--  <link rel="apple-touch-icon-precomposed" sizes="144x144"
          href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114"
          href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72"
          href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
  <link rel="shortcut icon" href="https://d3qcdigd1fhos0.cloudfront.net/blog/img/favicon.ico">
  <link rel='alternate' type='application/atom+xml' title='search.gov Atom feed' href='/all.atom' />
  <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
  <script src="/javascripts/bootstrap3-typeahead.js"></script>
 </head>

<body>
<nav class="navbar navbar-inverse navbar-fixed-top">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <a class="navbar-brand" href="/">
        <span class="search">Search.gov</span>
      </a>
    </div>
    <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">

      <form class="navbar-form navbar-left form-search" accept-charset="UTF-8" action="https://find.search.gov/search/" id="search-form" method="get">
        <div style="margin:0;padding:0;display:inline"><input name="utf8" type="hidden" value="&#x2713;" /></div>
        <input name="affiliate" id="affiliate" type="hidden" value="usasearch">
        <div class="input-group input-append">
          <label for="search-query" class="hide">Query</label>

          <input name="query" autocomplete="off"  type="text" class="typeahead form-control search-query" id="search-query" data-provide="typeahead" >
          <span class="input-group-btn">
            <button type="submit" class="btn btn-nav" aria-label="Left Align" id="search-button">
              <span class="glyphicon glyphicon-search" aria-hidden="true"></span>
            </button>
          </span>
        </div>
      </form>
      <div class="nav navbar-right">
          <a href="https://search.usa.gov/sites" class="navbar-brand"><i
              class="icon-user icon-white"></i>&nbsp;Login</a>
      </div>&nbsp;&nbsp;&nbsp;<div class="nav navbar-right">
          <a href="http://search.usa.gov/signup" class="navbar-brand">Sign up</a>
      </div>&nbsp;&nbsp;&nbsp;<div class="nav navbar-right">
          <a href="https://search.gov/status.html" class="navbar-brand">System Status</a>
      </div>
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>


<div class="col-md-offset-2 col-md-8  hidden-sm hidden-md hidden-lg">

  <form class="form-search" accept-charset="UTF-8" action="https://find.search.gov/search/" id="search-form" method="get">
    <input name="affiliate" id="affiliate" type="hidden" value="usasearch">
    <div class="input-group input-append">
      <label for="search-query" class="hide">Query</label>

      <input name="query" autocomplete="off"  type="text" class=" form-control search-query" id="search-query"  >
      <span class="input-group-btn">
        <button type="submit" class="btn btn-primary" aria-label="Left Align" id="search-button">
          <span class="glyphicon glyphicon-search" aria-hidden="true"></span>
        </button>
      </span>
    </div>
  </form>
</div>


<div class="container-fluid">
  <div class="col-md-8 col-md-offset-2">

    <!-- do not remove as used to parse in usasearch -->
    <main id="main-container">
      <!-- begin /blog/index.md content -->
<div class="articles">

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/status.html"&gt;Search.gov System Status&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;h2 id="current-status"&gt;Current Status&lt;/h2&gt;
</code></pre>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: 8:22am ET March 6, 2020 - ongoing<br />
Details:<br />
Update 12:40pm ET: The fix we implemented appears to have returned us to our normal working state, we are continuing to monitor.<br />
Original post 12:10pm ET: We’ve been experiencing intermittent errors throughout the morning, we are actively investigating. Updates will be posted every 30 minutes.</p>

    <h2 id="recent-events">Recent Events</h2>

    <p><img src="https://search.gov/img/red-status-system-down.png" width="15px" height="15px" alt="Outage" title="Outage" />    Outage <br />
Date: 7:02pm - 7:49pm ET March 5, 2020<br />
Details: Beginning 7:02pm ET, an outage at our hosting provider prevented us from distributing incoming requests to our servers. Their issue was resolved, but in addition, a database update by them to an unused feature caused our internal requests to fail. We applied an update to match their update, and service was restored.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: 8:00 pm October 29 - 10:15 am October 30, 2019<br />
Details: Beginning 8 PM ET 10/29, search sites using certain search features began returning errors for all searches. During our planned maintenance, our infrastructure host deployed routing settings to more servers than we requested, and this caused the error. After reverting the routing setting, all search sites are now behaving normally.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: August 31, 2019, 8:49am - 4:30pm ET <br />
Details: An issue at our cloud hosting provider caused new logins to the Admin Center to fail. Searches were working normally.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: August 20, 2019, 9:30pm ET - August 21 9:50am ET <br />
Details: An internal DNS issue caused our web index to return “No results” messages to some sites in their web results module. We have resolved the DNS issue and all systems are functioning normally.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: August 16, 2019, 10:30am - 3:30pm<br />
Details: We performed unplanned maintenance to alleviate issues on search result pages and the Admin Center.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: August 12, 2019, 10:30am - 2:00pm, and from 2:30pm - 5:50pm ET<br />
Details: We experienced two periods of intermittent errors and longer than usual load times, during high load to our results API. The load resulted in a reallocation process of our search indexes.</p>

    <p><img src="https://search.gov/img/orange-status-serious-degradation.png" width="15px" height="15px" alt="Serious Degradation" title="Serious Degradation" />    Serious Degradation<br />
Date: July 17, 2019, 8am - approx 10:30am <br />
Details: Server issues caused intermittent issues on search result pages (SERPs) and site Admin Centers.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation<br />
Date: July 16, 2019, 10:30am-11:30am <br />
Details: We conducted restarts of our servers to resolve memory issues. This resulted in intermittent degradation to search experiences.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation<br />
Date: July 15, 2019, approx 1:51pm-6:15pm <br />
Details: Memory issues in our back-end services caused intermittent issues when users attempted to complete searches.</p>

    <p><img src="https://search.gov/img/red-status-system-down.png" width="15px" height="15px" alt="Outage" title="Outage" />    Outage <br />
Date: July 10, 2019, approx 2:35pm-2:50pm ET<br />
Details: Deploy issues resulted in errors on search results pages (SERPs). We will address these issues before attempting future deploys.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: April 8, 2019, approx 8am-11:30am ET<br />
Details: We experienced heavy load to our DNS, which we addressed through updating settings.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: March 11, 2019, 9:49am-1:55pm ET <br />
Details: We experienced a transient issue with our back-end services which made some search data intermittently unavailable for the time indicated. No data was lost.</p>

    <p><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" />    Intermittent Degradation <br />
Date: January 30-31, 2019 <br />
Details: We experienced intermittent errors due to request queue volume. Errors resolved on retry. We performed maintenance on our primary index and increased the processing power for the machines that support this index.</p>

    <h2 id="legend">Legend</h2>

    <table>
  <thead>
    <tr>
      <th style="text-align: center">  Color  </th>
      <th style="text-align: left">Status                  </th>
      <th style="text-align: left">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img src="https://search.gov/img/green-status-normal-operations.png" width="15px" height="15px" alt="Fully Operational" title="Fully Operational" /></td>
      <td style="text-align: left">Fully Operational</td>
      <td style="text-align: left">Systems are operating normally.</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://search.gov/img/yellow-status-intermittent-operations.png" width="15px" height="15px" alt="Intermittent Degradation" title="Intermittent Degradation" /></td>
      <td style="text-align: left">Intermittent Degradation   </td>
      <td style="text-align: left">Periodic errors that resolve themselves on retry.</td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://search.gov/img/orange-status-serious-degradation.png" width="15px" height="15px" alt="Serious Degradation" title="Serious Degradation" /></td>
      <td style="text-align: left">Serious Degradation</td>
      <td style="text-align: left">One or more components of the Search.gov service is unavailable, but other services are fully operational.   </td>
    </tr>
    <tr>
      <td style="text-align: center"><img src="https://search.gov/img/red-status-system-down.png" width="15px" height="15px" alt="Outage" title="Outage" /></td>
      <td style="text-align: left">Outage</td>
      <td style="text-align: left">The Search.gov service is down.</td>
    </tr>
  </tbody>
</table>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/status"&gt;&lt;span class="label label-info"&gt;status&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/status.html"&gt;
    &lt;time datetime="2020-03-06"&gt;March 6, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/releases/february-2020.html"&gt;February 2020 Release Notes&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;h2 id="improvements"&gt;Improvements&lt;/h2&gt;
</code></pre>

    <ul>
  <li><strong>Account Deactivation Reminders:</strong> Search.gov users who have been inactive for more than 90 days must be deactivated, per security requirements. We will now send email reminders to log in prior to the 90 day mark to keep their accounts active.</li>
  <li><strong>Search.gov Account Signup:</strong> Users will need to complete all account information in order to manage their websites. On the back end, we have added new fields for first name and last name, which users will soon be seeing in their accounts. Where possible, these fields will be pre-filled, but in some cases, users will need to complete the Last Name field on their next login before proceeding.</li>
</ul>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/release-notes"&gt;&lt;span class="label label-info"&gt;release-notes&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  &lt;/br&gt;&lt;span&gt;Page last reviewed or updated:&lt;/span&gt;
  
  
  
  &lt;a href="/releases/february-2020.html"&gt;
    &lt;time datetime="2020-03-03"&gt;March 3, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/manual/how-search-engines-index-content-better-discoverability.html"&gt;How to get search engines to index the right content for better discoverability&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;p&gt;Website structure and content can have a significant impact on the ability of search engines to provide a good search experience. As a result, the Search Engine Optimization industry evolved to provide better understanding of these impacts and close critical gaps. Some elements on your website will actively hinder the search experience, and this post will show you how to target valuable content and exclude distractions.&lt;/p&gt;
</code></pre>

    <p>We’ve written a <a href="https://search.gov/manual/robotstxt.html">post about robots.txt files</a>, talking about high level inclusion and exclusion of content from search engines. There are other key tools you will want to employ on your website to further target the content on individual pages:</p>

    <ul>
  <li><a href="#main-element">The &lt;main&gt; element</a></li>
  <li><a href="#rel-canonical">Canonical links</a></li>
  <li><a href="#robots">Robots meta tags</a></li>
  <li>Or a combination of the above: <a href="#sample">Sample code structure for dynamic lists and an archived event</a></li>
</ul>

    <p><br />
<a id="main-element"></a></p>
    <h2 id="the-main-element">The <code>&lt;main&gt;</code> element</h2>

    <h3 id="targeting-particular-content-on-a-page">Targeting particular content on a page</h3>

    <p>A <code>&lt;main&gt;</code> element allows you to target content you want indexed by search engines. If a <code>&lt;main&gt;</code> element is present, the system will only collect the content inside the element. Be sure that the content you want indexed is inside of this element. If the element is closed too early, important content will not be indexed. Unless the system finds a <code>&lt;main&gt;</code> element demarcating where the primary content of the page is to be found, or other semantic section markers, repetitive content such as headers, footers, and sidebars can be picked up by search engines as part of a page’s content. We recommend adding <code>&lt;main&gt;</code> and other semantic elements such as <code>&lt;header&gt;</code>, <code>&lt;nav&gt;</code>, and <code>&lt;footer&gt;</code> to demarcate these sections and facilitate clean indexing.</p>

    <p>The element is implemented as a stand-alone tag:</p>

    <pre><code>&lt;body&gt;
Redundant header code and navigation elements, sidebars, etc.
&lt;main&gt;
&lt;h1&gt;This is your page title&lt;/h1&gt;
&lt;p&gt;This is the main text of your page
&lt;/main&gt;
Redundant footer code
Various scripts, etc.
&lt;/body&gt;
</code></pre>

    <p>The element can also take the form of a <code>&lt;div&gt;</code> with the role of main, though this approach is now outdated:</p>

    <pre><code>&lt;body&gt;
Redundant header code and navigation elements, sidebars, etc.
&lt;div role=”main”&gt;
&lt;h1&gt;This is your page title&lt;/h1&gt;
&lt;p&gt;This is the main text of your page
&lt;/div&gt;
Redundant footer code
Various scripts, etc.
&lt;/body&gt;
</code></pre>

    <p>If possible, open the <code>&lt;main&gt;</code> tag just ahead of the <code>&lt;H1&gt;</code> for your page title. If you use breadcrumbs on your site, <code>&lt;main&gt;</code> should be placed in between the breadcrumbs and the <code>&lt;H1&gt;</code> so that the repetitive text in the breadcrumb links will not be indexed.</p>

    <p>If no <code>&lt;main&gt;</code> element is present, we will omit the <code>&lt;nav&gt;</code> and <code>&lt;footer&gt;</code> elements. If none of these are present, the entire page will be scraped. Full-page scraping performs best for non-HTML file types, including PDFs and DOCs, so we encourage you to implement these semantic elements in your page templates to assist the search engines in understanding the structure of your site.</p>

    <p><br />
<a id="rel-canonical"></a></p>
    <h2 id="canonical-links">Canonical links</h2>
    <h3 id="declare-the-real-url-for-a-page">Declare the ‘real’ URL for a page</h3>

    <p>There are two good reasons to declare the URL for a given page: CMS sites can easily become crawler traps, and list views can generate urls that are unhelpful as search results.</p>

    <p>A crawler trap occurs when the engine falls into a loop of visiting, opening, and “discovering” pages that seem new, but are modifications on existing URLs. These URLs may have appended parameters such as tags, referring pages, Google Tag Manager tokens, page numbers, etc. Crawler traps tend to occur when your site can generate an infinite number of URLs. The crawler is ultimately unable to determine what constitutes the entirety of a site.
<code>&lt;link rel="canonical" href="https://www.example.gov/topic1" /&gt;</code></p>

    <p>By using a canonical link, shown above, you tell the crawler this is the real URL for the page despite parameters present in the URL when the page is opened. In the example above, even if a crawler opened the page with a URL like <code>https://example.gov/topic1?sortby=desc</code>, only <code>https://www.example.gov/topic1</code> will be captured by the search engine.</p>

    <p>Another important use-case for canonical links is the dynamic list. If the example above is a dynamic list of pages about Topic 1, it’s likely there will be pagination at the bottom of the page. This pagination dynamically separates items into distinct pages and generates urls like: <code>https://example.gov/topic1?page=3</code>. As new items are added to or removed from the list, there’s no guarantee that existing items will remain on a particular page. This behavior may frustrate users when a particular page no longer contains the item they want.</p>

    <p>Use a canonical link to limit the search engine to indexing only the first page of the list, which the user can then sort or move through as they choose. The individual items on the list are indexed separately and included in search results.</p>

    <p><br />
<a id="robots"></a></p>
    <h2 id="robots-meta-tags">Robots meta tags</h2>
    <h3 id="exclude-particular-pages-from-indexing-or-exclude-their-links-from-being-followed">Exclude particular pages from indexing, or exclude their links from being followed</h3>

    <p>There are individual pages on your websites that do not make good search results. This could be archived event pages, list views such as Recent Blog Posts, etc. Blocking individual pages on the <a href="https://search.gov/manual/robotstxt.html">robots.txt file</a> will be difficult if you don’t have easy access to edit the file Even if edits are easy, it could quickly lead to an unmanageably long <code>robots.txt</code>.</p>

    <p>It’s also important to note that search engines will pay attention to <code>Disallow</code> directives in <code>robots.txt</code> when crawling, but may not when accessing your URLs from other sources, like links from other sites or your sitemap. <strong>Search.gov will rely on robots meta tags when working off your sitemap to know what content you want searchable, and what you don’t want searchable.</strong></p>

    <p>To achieve best results for blocking indexing of particular pages, you’ll want to employ meta robots tags in the <code>&lt;head&gt;</code> of the pages you want to exclude from the search index.</p>

    <p>This example says not to index the page, but allows following the links on the page:</p>

    <p><code>&lt;meta name="robots" content="noindex" /&gt;</code></p>

    <p>This example says to index the page, but not follow any of the links on the page:</p>

    <p><code>&lt;meta name="robots" content="nofollow" /&gt;</code></p>

    <p>This example tells bots not to index the page, and not to follow any of the links on the page:</p>

    <p><code>&lt;meta name="robots" content="noindex, nofollow" /&gt;</code></p>

    <p>You can also add an X-Robots-Tag to you HTTP header response to control indexing for a given page. This requires deeper access to servers than our customers usually have themselves, so if you are interested in learning more, you can do so <a href="https://developers.google.com/search/reference/robots_meta_tag">here</a>  <i class="icon-external-link"><span>(External link)</span></i>.</p>

    <p>If you have content that should be indexed when it’s fresh, but needs to be removed from the index once it’s outdated, you’ll want to take a few actions:</p>

    <ul>
  <li>Once the page’s window of relevance is over, add a <code>&lt;meta name="robots" content="noindex" /&gt;</code> tag to the <code>&lt;head&gt;</code> of the page.</li>
  <li>Make sure the modified_time on the page is updated.</li>
  <li>Leave the item in the <a href="https://search.gov/manual/sitemaps.html">sitemap</a>, so that search engines will see the page was updated, revisit it, and see that the item should be removed from the index.</li>
</ul>

    <p><br />
<a id="sample"></a></p>
    <h2 id="sample-code-structure">Sample code structure</h2>

    <ul>
  <li><a href="#list1">Dynamic list 1: Topic landing page</a></li>
  <li><a href="#list2">Dynamic list 2: Posts tagged XYZ page</a></li>
  <li><a href="#past-event">Event from last month</a></li>
</ul>

    <p><a id="list1"></a></p>
    <h3 id="dynamic-list-1-topic-landing-page">Dynamic list 1: Topic landing page</h3>

    <p>The following code sample is for a dynamically generated list of pages on your site, where you want the landing page for the list to appear in search results.</p>

    <pre><code>&lt;head&gt;
&lt;title&gt;Unique title of the page&lt;/title&gt;
&lt;meta name="description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time." /&gt;
&lt;meta property="og:title" content="Unique title of the page" /&gt;
&lt;meta property="og:description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time. This could be the same or slightly different than the regular meta description." /&gt;
&lt;meta property=”article:published_time” content=”2018-09-28” /&gt;
&lt;meta property=”article:modified_time” content=”2018-09-28” /&gt;
&lt;link rel="canonical" href="https://www.example.gov/topic1" /&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;header&gt;Redundant header code&lt;/header&gt;
&lt;nav&gt;Navigation elements, sidebars, breadcrumbs, etc.&lt;/nav&gt;
&lt;main&gt;
&lt;h1&gt;Unique title of the page&lt;/h1&gt;
&lt;p&gt;This is the introductory text of the page. It tells people what they’ll find here, why the topic is important, etc. This text is within the main element, and so it will be used to retrieve this page in searches.
&lt;/main&gt;
Dynamically generated list of relevant pages
Pagination
&lt;footer&gt;Redundant footer code&lt;/footer&gt;
Various scripts, etc.
&lt;/body&gt;
</code></pre>

    <p><a id="list2"></a></p>
    <h3 id="dynamic-list-2-posts-tagged-xyz">Dynamic list 2: Posts tagged XYZ</h3>

    <p>The following code sample is for a dynamically generated list of pages on your site, where you do not want the list to appear in search results. In the case of pages tagged with a particular term, the pages themselves would be good search results, but the list of them would be just another click between the user and the content.</p>

    <p>Note: the description tags are still present in case someone links to this page in another system and that system wants to display a summary with the link.</p>

    <pre><code>&lt;head&gt;
&lt;title&gt;Unique title of the page&lt;/title&gt;
&lt;meta name="robots" content="noindex" /&gt;
&lt;meta name="description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time. Recommended max characters is 175." /&gt;
&lt;meta property="og:title" content="Unique title of the page" /&gt;
&lt;meta property="og:description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time. Recommended max characters is 175. This could be the same or slightly different than the regular meta description." /&gt;
&lt;meta property=”article:published_time” content=”2018-09-28” /&gt;
&lt;meta property=”article:modified_time” content=”2018-09-28” /&gt;
&lt;link rel="canonical" href="https://www.example.gov/posts-tagged-xyz" /&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;header&gt;Redundant header code&lt;/header&gt;
&lt;nav&gt;Navigation elements, sidebars, breadcrumbs, etc.&lt;/nav&gt;
&lt;main&gt;
&lt;h1&gt;Unique title of the page&lt;/h1&gt;
Dynamically generated list of relevant pages
&lt;/main&gt;
Pagination
&lt;footer&gt;Redundant footer code&lt;/footer&gt;
Various scripts, etc.
&lt;/body&gt;
</code></pre>

    <p><a id="past-event"></a></p>
    <h3 id="event-from-last-month">Event from last month</h3>

    <p>In the following example, an event page was published in June, and then updated the day after the event occurred. This update adds the <code>meta robots</code> tag, which declares the page should not be indexed, and links from the page should not be followed in future crawls. Again, the meta descriptions are retained in case of linking from other systems.</p>

    <pre><code>&lt;head&gt;
&lt;title&gt;Unique title of the page&lt;/title&gt;
&lt;meta name="robots" content="noindex, nofollow" /&gt;
&lt;meta name="description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time. Recommended max characters is 175." /&gt;
&lt;meta property="og:title" content="Unique title of the page" /&gt;
&lt;meta property="og:description" content="Some multi-sentence description of various things a person will find on this page. This is a great place to use different terms for the same thing, which is hopefully both plain language and keyword stuffing at the same time. Recommended max characters is 175. This could be the same or slightly different than the regular meta description." /&gt;
&lt;meta property=”article:published_time” content=”2018-06-04” /&gt;
&lt;meta property=”article:modified_time” content=”2018-08-13” /&gt;
&lt;link rel="canonical" href="https://www.example.gov/events/august-12-title-of-event" /&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;header&gt;Redundant header code&lt;/header&gt;
&lt;nav&gt;Navigation elements, sidebars, breadcrumbs, etc.&lt;/nav&gt;
&lt;main&gt;
&lt;h1&gt;Unique title of the page&lt;/h1&gt;
&lt;p&gt;This is the introductory text of the page. It tells people what they’ll find here, why the topic is important, etc. This text is within the main element, and so it will be used to retrieve this page in searches.
Specifics about the event.
&lt;/main&gt;
&lt;footer&gt;Redundant footer code&lt;/footer&gt;
Various scripts, etc.
&lt;/body&gt;
</code></pre>

    <h2 id="resources">Resources</h2>

    <ul>
  <li><a href="https://www.w3schools.com/tags/tag_main.asp">HTML &lt;main&gt; Tag</a>  <i class="icon-external-link"><span>(External link)</span></i> - accessed October 10, 2018.</li>
  <li><a href="https://developers.google.com/search/reference/robots_meta_tag">Robots meta tag and X-Robots-Tag HTTP header specifications
</a>  <i class="icon-external-link"><span>(External link)</span></i> - accessed October 10, 2018.</li>
  <li><a href="https://yoast.com/rel-canonical/">rel=canonical: the ultimate guide
</a>  <i class="icon-external-link"><span>(External link)</span></i> - accessed October 10, 2018.</li>
  <li><a href="https://support.google.com/webmasters/answer/139066?hl=en">Consolidate duplicate URLs: Define a canonical page for similar or duplicate pages</a>  <i class="icon-external-link"><span>(External link)</span></i> - accessed October 10, 2018.</li>
</ul>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/indexing"&gt;&lt;span class="label label-info"&gt;indexing&lt;/span&gt;&lt;/a&gt;
  
  &lt;a href="/tagged/seo"&gt;&lt;span class="label label-info"&gt;seo&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/manual/how-search-engines-index-content-better-discoverability.html"&gt;
    &lt;time datetime="2020-02-27"&gt;February 27, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/releases/january-2020.html"&gt;January 2020 Release Notes&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;h2 id="improvements"&gt;Improvements&lt;/h2&gt;
</code></pre>

    <ul>
  <li><strong>Rails Version:</strong> We upgraded our search-gov infrastructure to Rails 5.2.</li>
  <li><strong>Infrastructure Updates:</strong> We conducted a large portion of housekeeping on our codebase after the Login.gov implementation.</li>
</ul>

    <h2 id="fixes">Fixes</h2>

    <ul>
  <li><strong>YouTube Results:</strong> We addressed a bug that was impacting YouTube search results for some sites using Search.gov.</li>
</ul>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/release-notes"&gt;&lt;span class="label label-info"&gt;release-notes&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  &lt;/br&gt;&lt;span&gt;Page last reviewed or updated:&lt;/span&gt;
  
  
  
  &lt;a href="/releases/january-2020.html"&gt;
    &lt;time datetime="2020-02-14"&gt;February 14, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/manual/sitemaps.html"&gt;XML Sitemaps&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;p&gt;An &lt;a href="https://en.wikipedia.org/wiki/Sitemaps"&gt;XML sitemap&lt;/a&gt;  &lt;i class="icon-external-link"&gt;&lt;span&gt;(External link)&lt;/span&gt;&lt;/i&gt; is an XML formatted file containing a list of URLs on a website. An XML sitemap provides information that allows a search engine to index your website more intelligently, and to keep its search index up to date.&lt;/p&gt;
</code></pre>

    <p>Sitemaps tell search engines what URLs are on a website, and, if URLs are added as they are published, they tell the engines what new content needs to be picked up. They may also provide additional metadata about each URL, such as the last modified date, which signals to the engine to update the index record for that page.</p>

    <p>Search.gov uses sitemaps to tell us what URLs should be in our index and when a URL has been updated. Sitemaps are used in a similar way by <a href="https://support.google.com/webmasters/answer/156184">Google</a>  <i class="icon-external-link"><span>(External link)</span></i>, Bing, and and other search engines. <em>Having an xml sitemap will improve your Google SEO (search engine optimization).</em></p>

    <p>Example: <a href="https://search.gov/sitemap.xml">https://search.gov/sitemap.xml</a></p>

    <h2 id="what-content-should-be-on-xml-sitemap">What content should be on XML sitemap?</h2>

    <p>Some sitemaps are comprehensive, but for very large sites you may need to publish several sitemaps. Each sitemap should be no more than 50MB or 50,000 URLs, whichever comes first. You do not need to add URLs of content you want to remain unsearchable.</p>

    <p><strong>Note</strong> that an HTML formatted file listing the pages of a site is more akin to an index page, and is not the same as an XML sitemap. HTML files are human friendly, but not machine friendly, and Search engines need an xml formatted file in order to leverage the information for indexing work.</p>

    <h2 id="more-than-one-web-platform-use-multiple-sitemaps">More than one web platform? Use multiple sitemaps.</h2>

    <p>It’s common for agencies to use more than one platform to publish their websites. For instance, a CMS was launched, but some content is still on the legacy site’s platform. In this case, use available plugins for the CMS’s in your environment to auto-generate sitemaps for that content. Manually generate a sitemap for any static content. You can publish a <a href="https://www.sitemaps.org/protocol.html#index">sitemap index file</a>  <i class="icon-external-link"><span>(External link)</span></i> that lists the locations of all your specific sitemaps, or you can list all your sitemaps on your robots.txt file.</p>

    <h2 id="how-do-search-engines-find-my-sitemaps">How do search engines find my sitemap(s)?</h2>

    <p>Sitemaps (or the <a href="https://www.sitemaps.org/protocol.html#index">sitemap index</a>  <i class="icon-external-link"><span>(External link)</span></i>) should be listed in your site’s robots.txt file, i.e.:<br />
<code>Sitemap: https://www.agency.gov/sitemap_1.xml</code><br />
<code>Sitemap: https://www.agency.gov/sitemap_2.xml</code></p>

    <p>List the appropriate sitemap(s) for the domain or subdomain. <code>www.exampleagency.gov/robots.txt</code> would list sitemaps for content in the <code>www</code> subdomain, while <code>forms.exampleagency.gov/robots.txt</code> would list sitemaps for the <code>forms</code> subdomain.</p>

    <p>Read more about <a href="https://search.gov/blog/robotstxt.html">robots.txt files</a>, and take a look at ours: <a href="https://search.gov/robots.txt">https://search.gov/robots.txt</a></p>

    <h2 id="what-should-my-xml-sitemap-look-like">What should my XML sitemap look like?</h2>

    <p>Please refer to the official <a href="https://www.sitemaps.org/protocol.html">sitemaps protocol</a>  <i class="icon-external-link"><span>(External link)</span></i> for full information on how a sitemap should be structured.</p>

    <p>When publishing your sitemap, be sure it begins with an <code>&lt;xml&gt;</code> declaration, and that the URLs are enclosed in opening and closing tags. To take a simplified example:</p>

    <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;urlset&gt;
&lt;url&gt;
&lt;loc&gt;https://exampleagency.gov/blog/file1.html&lt;/loc&gt;
&lt;lastmod&gt;2018-03-19T00:00:00+00:00&lt;/lastmod&gt;
&lt;/url&gt;
&lt;url&gt;
&lt;loc&gt;https://exampleagency.gov/policy/new-policy.html&lt;/loc&gt;
&lt;lastmod&gt;2018-03-27T00:00:00+00:00&lt;/lastmod&gt;
&lt;/url&gt;
&lt;/urlset&gt;
</code></pre>
    <p>If you use multiple sitemaps, then you’ll need to use a <a href="https://www.sitemaps.org/protocol.html#index">sitemap index</a>  <i class="icon-external-link"><span>(External link)</span></i>, along these lines:</p>

    <pre><code>&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sitemapindex&gt;
&lt;sitemap&gt;https://exampleagency.gov/sitemap.xml?page=1&lt;/sitemap&gt;
&lt;sitemap&gt;https://exampleagency.gov/sitemap.xml?page=2&lt;/sitemap&gt;
&lt;/sitemapindex&gt;
</code></pre>
    <p>Importantly, be sure that any <a href="https://www.sitemaps.org/protocol.html#escaping">special characters in your URLs are escaped</a>  <i class="icon-external-link"><span>(External link)</span></i> so the search engines will know how to read them.</p>

    <h2 id="what-metadata-does-searchgov-require-for-each-xml-sitemap-url">What metadata does Search.gov require for each XML sitemap URL?</h2>

    <p>The sitemap protocol defines <a href="https://www.sitemaps.org/protocol.html#xmlTagDefinitions">required and optional XML tags</a>  <i class="icon-external-link"><span>(External link)</span></i> for each URL. We recommend including the <code>&lt;lastmod&gt;</code> value (the date of last modification of the file) whenever possible, to indicate when a file has been updated and needs to be re-indexed.</p>

    <p>We do not have plans to support the <code>&lt;priority&gt;</code> tag, which is <a href="https://www.seroundtable.com/google-priority-change-frequency-xml-sitemap-20273.html">no longer used</a>  <i class="icon-external-link"><span>(External link)</span></i> by search engines like Google. We may support the <code>&lt;changefreq&gt;</code> tag in the future, but the <code>&lt;lastmod&gt;</code> tag is more accurate and supported by more search engines.</p>

    <h2 id="how-can-i-create-an-xml-sitemap">How can I create an XML sitemap?</h2>

    <p>Most content management systems provide tools to generate a sitemap and keep it updated. Below are some tools that we recommend:</p>

    <h3 id="drupal">Drupal</h3>
    <p><a href="https://www.drupal.org/project/xmlsitemap">XML Sitemap Module</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h3 id="wordpress">Wordpress</h3>
    <p><a href="https://wordpress.org/plugins/wordpress-seo/">Yoast SEO Plugin</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><a href="https://wordpress.org/plugins/google-sitemap-generator/">Google Sitemap Plugin</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h3 id="wagtail">Wagtail</h3>
    <p><a href="http://docs.wagtail.io/en/latest/reference/contrib/sitemaps.html">Sitemap Generator</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h3 id="github-pages-jekyll">Github Pages (Jekyll)</h3>
    <p><a href="https://help.github.com/articles/sitemaps-for-github-pages/">Jekyll Sitemap gem</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h3 id="online-generators">Online generators</h3>
    <p>(Note: free online generators often have a limit to the number of URLs they will include, and do not always generate the most accurate sitemaps. Use them only as a last resort.)</p>

    <p><a href="https://freesitemapgenerator.com">Free Sitemap Generator</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><a href="http://www.web-site-map.com/">Web Sitemap</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h2 id="sitemap-checklist">Sitemap checklist</h2>
    <p><i class="icon-check"></i> 1. One or more sitemaps have been created</p>

    <p><i class="icon-check"></i> 2. The URLs in the sitemap have been reviewed (clean URLs, only includes URLs that should be searchable)</p>

    <p><i class="icon-check"></i> 3. Each sitemap’s XML format has been <a href="https://www.websiteplanet.com/webtools/sitemap-validator/">validated</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><i class="icon-check"></i> 4. Each sitemap (or a sitemap index) is listed in the site’s robots.txt file</p>

    <h2 id="additional-resources">Additional Resources:</h2>
    <p><a href="https://www.sitemaps.org/">Official Documentation from Sitemaps.org</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><a href="https://support.google.com/webmasters/answer/183668?hl=en&amp;ref_topic=4581190">Google’s guide to building a sitemap</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><a href="https://www.websiteplanet.com/webtools/sitemap-validator/">Sitemap validator</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h2 id="more-questions">More questions?</h2>
    <p>If you have questions that aren’t answered here, <a href="mailto:search@support.digitalgov.gov">email us</a>. We’ll also keep updating this page over time.</p>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/sitemaps"&gt;&lt;span class="label label-info"&gt;sitemaps&lt;/span&gt;&lt;/a&gt;
  
  &lt;a href="/tagged/indexing"&gt;&lt;span class="label label-info"&gt;indexing&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/manual/sitemaps.html"&gt;
    &lt;time datetime="2020-01-17"&gt;January 17, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/manual/robotstxt.html"&gt;Robots.txt Files&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;p&gt;A &lt;code&gt;/robots.txt&lt;/code&gt; file is a text file that instructs automated web bots on how to crawl and/or index a website. Web teams use them to provide information about what site directories should or should not be crawled, how quickly content should be accessed, and which bots are welcome on the site.&lt;/p&gt;
</code></pre>

    <h2 id="what-should-my-robotstxt-file-look-like">What should my robots.txt file look like?</h2>
    <p>Please refer to the <a href="http://www.robotstxt.org/robotstxt.html">robots.txt protocol</a>  <i class="icon-external-link"><span>(External link)</span></i> for detailed information on how and where to create your robots.txt. Key points to keep in mind:</p>

    <ul>
  <li>The file must be located at the root of the domain, and each subdomain needs its own file.</li>
  <li>The robots.txt protocol is case sensitive.</li>
  <li>It’s easy to accidentally block crawling of everything
    <ul>
      <li><code>Disallow: /</code> means disallow everything</li>
      <li><code>Disallow:  </code> means disallow nothing, thus allowing everything</li>
      <li><code>Allow: /</code> means allow everything</li>
      <li><code>Allow:  </code> means allow nothing, thus disallowing everything</li>
    </ul>
  </li>
  <li>The instructions in robots.txt are guidance for bots, not binding requirements.</li>
</ul>

    <h2 id="how-can-i-optimize-my-robotstxt-for-searchgov">How can I optimize my robots.txt for Search.gov?</h2>

    <h3 id="crawl-delay">Crawl delay</h3>
    <p>A robots.txt file may specify a “crawl delay” directive for one or more user agents, which tells a bot how quickly it can request pages from a website. For example, a crawl delay of 10 specifies that a crawler should not request a new page more than every 10 seconds.</p>

    <pre><code>  500,000 URLs
     x 10 seconds between requests
5,000,000 seconds for all requests

5,000,000 seconds = 58 days to index the site once.
</code></pre>

    <p>We recommend a crawl-delay of 2 seconds for our <code>usasearch</code> user agent, and setting a higher crawl delay for all other bots. The lower the crawl delay, the faster Search.gov will be able to index your site. In the robots.txt file, it would look like this:</p>

    <pre><code>User-agent: usasearch  
Crawl-delay: 2

User-agent: *
Crawl-delay: 10
</code></pre>

    <h3 id="xml-sitemaps">XML Sitemaps</h3>
    <p>Your robots.txt file should also list one or more of your <a href="https://search.gov/blog/sitemaps.html">XML sitemaps</a>. For example:</p>

    <pre><code>Sitemap: https://www.exampleagency.gov/sitemap.xml
Sitemap: https://www.exampleagency.gov/independent-subsection-sitemap.xml
</code></pre>
    <ul>
  <li>Only list sitemaps for the domain matching where the robots.txt file is. A different subdomain’s sitemap should be listed on that subdomain’s robots.txt.</li>
</ul>

    <h3 id="allow-only-the-content-that-you-want-searchable">Allow only the content that you want searchable</h3>
    <p>We recommend disallowing any directories or files that should not be searchable. For example:</p>

    <pre><code>Disallow: /archive/
Disallow: /news-1997/
Disallow: /reports/duplicative-page.html
</code></pre>

    <ul>
  <li>Note that if you disallow a directory after it’s been indexed by a search engine, this may not trigger a removal of that content from the index. You’ll need to go into the search engine’s webmaster tools to request removal.</li>
  <li>Also note that search engines may index individual pages within a disallowed folder if the search engine learns about the URL from a non-crawl method, like a link from another site or your sitemap. To ensure a given page is not searchable, set a <a href="/blog/how-search-engines-index-content-better-discoverability.html#robots">robots meta tag</a> on that page.</li>
</ul>

    <h3 id="customize-settings-for-different-bots">Customize settings for different bots</h3>
    <p>You can set different permissions for different bots. For example, if you want us to index your archived content but don’t want Google or Bing to index it, you can specify that:</p>

    <pre><code>User-agent: usasearch  
Crawl-delay: 2
Allow: /archive/

User-agent: *
Crawl-delay: 10
Disallow: /archive/
</code></pre>

    <h2 id="robotstxt-checklist">Robots.txt checklist</h2>
    <p><i class="icon-check"></i> 1. A robots.txt file has been created in the site’s root directory (<code>https://exampleagency.gov/robots.txt</code>)</p>

    <p><i class="icon-check"></i> 2. The robots.txt file disallows any directories and files that automated bots should not crawl</p>

    <p><i class="icon-check"></i> 3. The robots.txt file lists one or more <a href="https://search.gov/blog/sitemaps.html">XML sitemaps</a></p>

    <p><i class="icon-check"></i> 4. The robots.txt file format has been <a href="https://www.websiteplanet.com/webtools/sitemap-validator/">validated</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <h2 id="additional-resources">Additional Resources</h2>
    <p><a href="https://yoast.com/ultimate-guide-robots-txt/">Yoast SEO’s Ultimate Guide to Robots.txt</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <p><a href="https://support.google.com/webmasters/answer/6062608?hl=en&amp;ref_topic=6061961">Google’s “Learn about robots.txt files”</a>  <i class="icon-external-link"><span>(External link)</span></i></p>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/how-to"&gt;&lt;span class="label label-info"&gt;how-to&lt;/span&gt;&lt;/a&gt;
  
  &lt;a href="/tagged/manage-content"&gt;&lt;span class="label label-info"&gt;manage-content&lt;/span&gt;&lt;/a&gt;
  
  &lt;a href="/tagged/indexing"&gt;&lt;span class="label label-info"&gt;indexing&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  &lt;/br&gt;&lt;span&gt;Page last reviewed or updated:&lt;/span&gt;
  
  
  
  
  &lt;a href="/manual/robotstxt.html"&gt;
    &lt;time datetime="2020-01-17"&gt;January 17, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/releases/december-2019.html"&gt;December 2019 Release Notes&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;h2 id="improvements"&gt;Improvements&lt;/h2&gt;
</code></pre>

    <ul>
  <li><strong>Rails:</strong> We upgraded our application to a new version of Rails</li>
  <li><strong>Security:</strong> We upgraded some components of our system to ensure more secure infrastructure.</li>
  <li><strong>Elasticsearch:</strong> We continued work on upgrading our analytics engine’s version of Elasticsearch.</li>
</ul>

    <h2 id="fixes">Fixes</h2>

    <ul>
  <li><strong>Input Improvements:</strong> We added input validation to a couple fields that originally allowed off-format submissions.</li>
</ul>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/release-notes"&gt;&lt;span class="label label-info"&gt;release-notes&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  &lt;/br&gt;&lt;span&gt;Page last reviewed or updated:&lt;/span&gt;
  
  
  
  &lt;a href="/releases/december-2019.html"&gt;
    &lt;time datetime="2020-01-15"&gt;January 15, 2020&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/blog/annual-reviews.html"&gt;Search.gov Annual Reviews&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;h2 id="what-people-are-trying-to-do-on-government-websites-in-their-own-words"&gt;What people are trying to do on government websites, in their own words&lt;/h2&gt;
</code></pre>

    <p>Search.gov is pleased to present our annual reviews - high level reports that show, in their own words, what the public has been searching for across government websites.</p>

    <p>From common needs for forms and process status checks, to the latest information on the news of the year, or vulnerable moments seeking help in times of need, Search.gov is there, connecting people with their government.</p>

    <ul>
  <li>
    <p><a href="https://search.gov/blog/2019-annual-review.html">2019</a></p>
  </li>
  <li>
    <p><a href="https://search.gov/blog/2018-annual-review.html">2018</a></p>
  </li>
  <li>
    <p><a href="https://search.gov/blog/2017-annual-review.html">2017</a></p>
  </li>
</ul>

    <p>Changes in the data over time reflect which agencies use our service, and major news in a given year. The data represents only those websites that use Search.gov, and does not include data on searches run on commercial search engines.</p>

    <p>If you have questions or comments, please feel free to <a href="mailto:search@support.digitalgov.gov">reach out</a> to our team.</p>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/annual-review"&gt;&lt;span class="label label-info"&gt;annual-review&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/blog/annual-reviews.html"&gt;
    &lt;time datetime="2019-12-20"&gt;December 20, 2019&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/blog/2019-annual-review.html"&gt;2019 in Review&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;p&gt;&lt;span style="float:right;"&gt;&lt;img src="/files/2019_annual_review_small.png" alt="Top Topics in 2019 poster, small version. Following is a link to a larger PDF version. The poster shows a set of 16 pie charts, one for each top topic. The pie charts show the details of the top 25 queries run in 2019 for that topic. The chart labels are accessible, but unfortunately the pie charts are image-only. Text versions are available on request, reach out to the Search team for assistnce." style="width:260px;height:300px;" /&gt;&lt;br /&gt; &lt;a href="/files/2019_annual_review_large.pdf"&gt;Open large version&lt;/a&gt; &lt;/span&gt; &lt;br /&gt;&lt;/p&gt;
</code></pre>

    <h4 id="295916305-queries"><strong>295,916,305</strong> queries</h4>
    <h4 id="2000-gov-websites"><strong>~2,000 gov</strong> websites</h4>
    <h4 id="16-top-topics"><strong>16</strong> top topics</h4>
    <h4 id="365-days-of-connecting-people-with-what-they-need"><strong>365</strong> days of connecting people with what they need.</h4>

    <p><br /></p>

    <h2 id="system-highlights">System Highlights</h2>

    <ul>
  <li>Added ranking factors to our search algorithm (popularity, freshness)</li>
  <li>Released help documentation about site launches and the indexing process</li>
  <li>Added a sitelimit feature to provide an alternative to Collection search scoping</li>
  <li>Added indexing coverage for javascript pages and other metadata-only files.</li>
  <li>Increased security on user accounts
    <ul>
      <li>automatic disabling after 90 days of inactivity</li>
      <li>MFA login through Login.gov</li>
    </ul>
  </li>
  <li>Upgraded Ruby</li>
  <li>Upgraded Rails</li>
  <li>Upgraded Elasticsearch</li>
  <li>Upgraded jQuery</li>
  <li>Increased server capacity and processing power</li>
  <li>Added query caching on our primary index to improve response time</li>
  <li>Expanded our Elasticsearch system twice</li>
</ul>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/annual-review"&gt;&lt;span class="label label-info"&gt;annual-review&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/blog/2019-annual-review.html"&gt;
    &lt;time datetime="2019-12-20"&gt;December 20, 2019&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

  <article class="article">

    <pre><code>&lt;h1&gt;
  &lt;a href="/blog/2018-annual-review.html"&gt;2018 in Review&lt;/a&gt;
&lt;/h1&gt;


&lt;div class='post-content'&gt;
  &lt;p&gt;&lt;span style="float:right;"&gt;&lt;img src="/files/2018_annual_review_small.png" alt="Top Topics in 2018 poster, small version. Following is a link to a larger PDF version. The poster shows a set of 13 pie charts, one for each top topic. The pie charts show the details of the top 25 queries run in 2019 for that topic. The chart labels are accessible, but unfortunately the pie charts are image-only. Text versions are available on request, reach out to the Search team for assistnce." style="width:260px;height:300px;" /&gt;&lt;br /&gt; &lt;a href="/files/2018_annual_review_large.pdf"&gt;Open large version&lt;/a&gt; &lt;/span&gt; &lt;br /&gt;&lt;/p&gt;
</code></pre>

    <h4 id="272534578-queries"><strong>272,534,578</strong> queries</h4>
    <h4 id="2000-gov-websites"><strong>~2,000 gov</strong> websites</h4>
    <h4 id="13-top-topics"><strong>13</strong> top topics</h4>
    <h4 id="365-days-of-connecting-people-with-what-they-need"><strong>365</strong> days of connecting people with what they need.</h4>

    <p><br />
<br />
<br />
<br />
<br />
<br />
<br />
<br /></p>

    <pre><code>&lt;/div&gt;

&lt;div class='tags'&gt;
  
  &lt;a href="/tagged/annual-review"&gt;&lt;span class="label label-info"&gt;annual-review&lt;/span&gt;&lt;/a&gt;
  
&lt;/div&gt;

&lt;div class='time'&gt;
  
  
  
  
  &lt;a href="/blog/2018-annual-review.html"&gt;
    &lt;time datetime="2019-12-20"&gt;December 20, 2019&lt;/time&gt;
  &lt;/a&gt;
&lt;/div&gt;
</code></pre>

  </article>

</div>

<ul class="pager">
  <li class="next">
    <a href="/blog/page2">Older &rarr;</a>
  </li>
</ul>
<!-- end /blog/index.md content -->

    </main>
  </div>
</div>

<footer class="footer">
  <div class="container">
    <p><a href="mailto:search@support.digitalgov.gov">Email us</a> or call us at 202-969-7426</p>
    <p> An Official Website of the U.S. Government <br />
      <a href="https://www.gsa.gov/portal/category/25729">Technology Transformation Service</a>, <a href="https://www.gsa.gov/portal/category/100000">U.S. General Services Administration</a>
    </p>
    <ul class="footer-links list-unstyled list-inline">
      <li><a href="https://www.usa.gov">USA.gov</a></li>
      <li class="muted">&bull;</li>
      <li><a href="/tos.html">Terms of Service</a></li>
      <li class="muted">&bull;</li>
      <li><a href="https://www.digitalgov.gov/about/policies/">Site Policies</a></li>
      <li class="muted">&bull;</li>
      <li><a href="/developer/">Developers</a></li>
    </ul>
  </div>
</footer>

<script type="text/javascript">
  //<![CDATA[
  var usasearch_config = { siteHandle:"usasearch" };

jQuery(document).ready(function() {
  $('.typeahead').typeahead({
      source: function (query, process) {
          return $.get('https://search.usa.gov/sayt?name=usasearch&q=' + query, function (data) {
              return process(data);
          });
      }, minLength: 2
  });
})


  // var script = document.createElement("script");
  // script.type = "text/javascript";
  // script.src = "https://search.usa.gov/javascripts/remote.loader.js";
  // document.getElementsByTagName("head")[0].appendChild(script);

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31302465-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

  //]]>
</script>

<script async type="text/javascript" src="https://dap.digitalgov.gov/Universal-Federated-Analytics-Min.js?agency=GSA" id="_fed_an_ua_tag"></script>


</body>
</html>
